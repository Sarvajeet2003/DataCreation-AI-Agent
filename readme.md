# Agentic Test Generation

This project implements an AI-driven test generation pipeline using a multi-agent approach. It consists of four specialized agents that work together to search for, generate, analyze, and enhance datasets for test case generation.

## Project Overview

The Agentic Test Generation system uses a pipeline of four specialized AI agents to generate high-quality test datasets:

### Agent 1: Dataset-Focused Web Search Agent

Agent 1 searches the web for relevant datasets based on user queries. It extracts and processes content from web pages to find suitable datasets for the specified domain.

**Key Features:**
- Web search functionality using requests and BeautifulSoup
- Content extraction and cleaning
- User context analysis to understand dataset requirements
- Dataset relevance scoring and ranking

### Agent 2: AI-Driven Dataset Generation Agent

Agent 2 analyzes Agent 1's output and creates synthetic datasets with realistic data using AI-driven approaches without hardcoded domain logic.

**Key Features:**
- AI-based data schema creation
- Synthetic dataset generation based on domain requirements
- Realistic data generation using Faker library
- CSV file export for further processing

### Agent 3: AI-Driven Dataset Analysis & Quality Assessment Agent

Agent 3 analyzes the datasets generated by Agent 2, detects errors, and provides intelligent feedback on data quality.

**Key Features:**
- Comprehensive dataset analysis
- Quality issue detection and categorization
- Cross-dataset analysis
- AI-driven assessment and recommendations

### Agent 4: AI-Driven Dataset Enhancement & Issue Resolution Agent

Agent 4 loads Agent 2's generated datasets, reads Agent 3's quality analysis reports, validates logical correctness, and fixes identified issues.

**Key Features:**
- Logical validation of synthetic data
- Issue resolution based on Agent 3's findings
- Data enhancement for improved quality
- Final dataset output for test case generation

## Setup and Installation

### Prerequisites

- Python 3.8 or higher
- Google Gemini API key

### Installation

1. Clone the repository:
   ```bash
   git clone https://github.com/UKsarvajeeth/Agentic-Test-Generation.git
   cd Agentic-Test-Generation
   ```

2. Install the required dependencies:
   ```bash
   pip install -r requirements.txt
   ```

3. Set up your environment variables:
   ```bash
   export GEMINI_API_KEY="your_gemini_api_key_here"
   ```
   Or create a `.env` file in the project root with:
   ```
   GEMINI_API_KEY=your_gemini_api_key_here
   ```

## Usage

### Running the Agents

Each agent can be run individually in sequence:

1. **Agent 1: Web Search**
   ```bash
   python agent1_websearch.py
   ```
   This will generate `agent1_output.json` with search results.

2. **Agent 2: Dataset Generation**
   ```bash
   python agent2.py
   ```
   This will read `agent1_output.json` and generate synthetic datasets in the `agent2_output` directory.

3. **Agent 3: Quality Analysis**
   ```bash
   python agent3.py
   ```
   This will analyze the datasets in `agent2_output` and generate quality reports in the `agent3_output` directory.

4. **Agent 4: Enhancement & Issue Resolution**
   ```bash
   python agent4.py
   ```
   This will enhance the datasets based on Agent 3's analysis and save the results in the `agent4_output` directory.

### Example Workflow

1. Start with a query about a specific domain for which you need test data
2. Agent 1 searches the web for relevant datasets
3. Agent 2 generates synthetic datasets based on the search results
4. Agent 3 analyzes the quality of the generated datasets
5. Agent 4 enhances the datasets by fixing issues identified by Agent 3
6. Use the final enhanced datasets for your test case generation

## Project Structure

```
├── agent1_websearch.py       # Agent 1: Dataset-Focused Web Search Agent
├── agent1_output.json        # Output from Agent 1
├── agent2.py                 # Agent 2: AI-Driven Dataset Generation Agent
├── agent2_output/            # Output directory for Agent 2
├── agent3.py                 # Agent 3: AI-Driven Dataset Analysis Agent
├── agent3_output/            # Output directory for Agent 3
├── agent4.py                 # Agent 4: AI-Driven Dataset Enhancement Agent
├── agent4_output/            # Output directory for Agent 4
└── requirements.txt          # Project dependencies
```

## Dependencies

The project relies on the following key libraries:
- langchain and langchain-google-genai for LLM interactions
- pandas and numpy for data manipulation
- requests and beautifulsoup4 for web scraping
- faker for generating realistic synthetic data
- google-generativeai for direct Gemini API access