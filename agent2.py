"""
Agent 2: AI-Driven Dataset Generation Agent
Analyzes Agent 1's output, creates data schemas, and generates logical data
using AI-driven approaches without hardcoded domain logic.
"""

import json
import pandas as pd
import numpy as np
import logging
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, asdict, field
from datetime import datetime
import os
import random
from faker import Faker
from pathlib import Path
import re

from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.schema import HumanMessage, SystemMessage

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize Faker for generating realistic data
fake = Faker()

def convert_numpy_types(obj):
    """Convert numpy types to Python native types for JSON serialization"""
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, list):
        return [convert_numpy_types(item) for item in obj]
    elif isinstance(obj, pd.Series):
        return obj.tolist()
    elif pd.isna(obj):
        return None
    return obj

class CustomJSONEncoder(json.JSONEncoder):
    """Custom JSON encoder for numpy types and pandas objects"""
    def default(self, obj):
        if isinstance(obj, (np.integer, np.int64, np.int32)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64, np.float32)):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        elif isinstance(obj, pd.Series):
            return obj.tolist()
        elif isinstance(obj, pd.DataFrame):
            return obj.to_dict('records')
        elif pd.isna(obj):
            return None
        elif isinstance(obj, datetime):
            return obj.isoformat()
        return super().default(obj)

@dataclass
class GeneratedDataset:
    """Represents a dataset generated by Agent 2"""
    name: str
    description: str
    columns: List[str]
    file_path: str = ""
    row_count: int = 0
    
    def to_dict(self):
        return convert_numpy_types(asdict(self))

@dataclass
class Agent2Output:
    """Complete output from Agent 2"""
    original_context: Dict[str, Any]
    generated_datasets: List[GeneratedDataset]
    timestamp: str = field(default_factory=lambda: datetime.now().isoformat())
    
    def to_dict(self):
        return {
            'original_context': convert_numpy_types(self.original_context),
            'generated_datasets': [ds.to_dict() for ds in self.generated_datasets],
            'timestamp': self.timestamp
        }

class AIDatasetGenerator:
    """AI-Driven Dataset Generation Agent"""
    
    def __init__(self, gemini_api_key: str, output_dir: str = "agent2_output"):
        self.gemini_api_key = gemini_api_key
        self.output_dir = output_dir
        
        # Create output directory if it doesn't exist
        os.makedirs(output_dir, exist_ok=True)
        
        # Initialize Gemini LLM
        self.llm = ChatGoogleGenerativeAI(
            google_api_key=gemini_api_key,
            model="gemini-1.5-flash",
            temperature=0.2
        )
        
        logger.info("AI Dataset Generator initialized")
    
    async def process_agent1_output(self, agent1_output_path: str) -> Agent2Output:
        """Process Agent 1's output to generate datasets"""
        start_time = datetime.now()
        
        # Load Agent 1's output
        with open(agent1_output_path, 'r') as f:
            agent1_data = json.load(f)
        
        logger.info("Loaded Agent 1 output, analyzing for dataset generation")
        
        # Use AI to analyze Agent 1's output and create a data generation plan
        data_schema = await self._ai_create_data_schema(agent1_data)
        
        # Generate dataset based on the AI-created schema
        generated_datasets = await self._ai_generate_datasets(data_schema, agent1_data)
        
        # Save datasets as CSV files
        for dataset in generated_datasets:
            self._save_dataset_as_csv(dataset)
        
        # Create output for Agent 3
        result = Agent2Output(
            original_context=agent1_data,
            generated_datasets=generated_datasets
        )
        
        # Save Agent 2 output as JSON
        output_path = os.path.join(self.output_dir, "agent2_ai_output.json")
        with open(output_path, 'w') as f:
            json.dump(result.to_dict(), f, indent=2, cls=CustomJSONEncoder)
        
        logger.info(f"Agent 2 processing completed in {(datetime.now() - start_time).total_seconds():.2f}s")
        logger.info(f"Generated {len(generated_datasets)} datasets")
        
        return result
    
    async def _ai_create_data_schema(self, agent1_data: Dict[str, Any]) -> Dict[str, Any]:
        """Use AI to create a data schema based on Agent 1's output"""
        logger.info("Using AI to create data schema from Agent 1's output")
        
        user_context = agent1_data.get('user_context', {})
        available_datasets = agent1_data.get('available_datasets', [])
        recommendations = agent1_data.get('recommendations', {})
        
        # Create a prompt for the AI to analyze the data and create a schema
        system_prompt = """
You are an expert data scientist specializing in dataset schema design.
Analyze the provided information from a web search agent and create a comprehensive data schema.

Your task is to:
1. Understand the user's query and domain
2. Analyze available datasets found by the search agent
3. Design an optimal schema for synthetic data generation
4. Determine appropriate data types and relationships
5. Specify value ranges and distributions that make sense for the domain

Return a JSON object with the following structure:
{
  "dataset_name": "Name of the dataset",
  "description": "Detailed description of the dataset",
  "columns": ["list", "of", "column", "names"],
  "column_details": {
    "column_name": {
      "type": "data type (string, int, float, date, categorical, boolean, id)",
      "description": "description of this column",
      "value_range": "description of valid values or range",
      "distribution": "description of how values should be distributed",
      "relationships": "description of relationships with other columns if any"
    }
  },
  "row_count": recommended number of rows to generate,
  "domain_specific_rules": ["list", "of", "domain", "specific", "rules"],
  "data_generation_guidance": "guidance for generating realistic data"
}
"""
        
        # Prepare the user query with context from Agent 1
        user_query = f"""
User's original query: {user_context.get('original_query', '')}
Domain: {user_context.get('domain', 'general')}
Intent: {user_context.get('intent', '')}

Key entities: {', '.join(user_context.get('key_entities', []))}

Expected columns: {', '.join(user_context.get('expected_columns', []))}

Data requirements: {json.dumps(user_context.get('data_requirements', []), indent=2)}

Available datasets found by search agent:
{json.dumps(available_datasets, indent=2)}

Recommendations:
{json.dumps(recommendations, indent=2)}

Create a comprehensive data schema for generating synthetic data that fulfills the user's requirements.
"""
        
        messages = [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_query)
        ]
        
        response = await self.llm.ainvoke(messages)
        
        # Extract JSON from response
        json_match = re.search(r'\{.*\}', response.content, re.DOTALL)
        if json_match:
            schema = json.loads(json_match.group())
            logger.info(f"AI created schema for dataset: {schema.get('dataset_name', 'Unknown')}")
            return schema
        else:
            logger.warning("Failed to extract schema from AI response, using fallback approach")
            # Fallback to a basic schema if AI fails
            return self._create_fallback_schema(agent1_data)
    
    def _create_fallback_schema(self, agent1_data: Dict[str, Any]) -> Dict[str, Any]:
        """Create a basic fallback schema if AI schema creation fails"""
        user_context = agent1_data.get('user_context', {})
        
        # Extract expected columns from user context
        expected_columns = user_context.get('expected_columns', [])
        
        # Create a basic schema
        schema = {
            'dataset_name': f"Synthetic{user_context.get('domain', '').capitalize()}Dataset",
            'description': f"Synthetic dataset for {user_context.get('original_query', 'research')}",
            'columns': expected_columns,
            'column_details': {},
            'row_count': 10,
            'domain_specific_rules': [],
            'data_generation_guidance': "Generate realistic data based on the domain."
        }
        
        return schema
    
    async def _ai_generate_datasets(self, schema: Dict[str, Any], agent1_data: Dict[str, Any]) -> List[GeneratedDataset]:
        """Use AI to generate datasets based on the schema"""
        logger.info(f"Using AI to generate dataset: {schema['dataset_name']}")
        
        # Create a pandas DataFrame with the specified schema
        df = await self._ai_generate_data(schema, agent1_data)
        
        # Create dataset metadata
        dataset = GeneratedDataset(
            name=schema['dataset_name'],
            description=schema['description'],
            columns=schema['columns'],
            row_count=len(df)
        )
        
        # Set file path
        dataset.file_path = os.path.join(self.output_dir, f"{dataset.name}.csv")
        
        # Store DataFrame for later saving
        self._dataframes = {dataset.name: df}
        
        return [dataset]
    
    async def _ai_generate_data(self, schema: Dict[str, Any], agent1_data: Dict[str, Any]) -> pd.DataFrame:
        """Use AI to generate synthetic data based on the schema"""
        columns = schema['columns']
        column_details = schema.get('column_details', {})
        # Override row_count to always be 50 regardless of schema recommendation
        row_count = 50
        domain_rules = schema.get('domain_specific_rules', [])
        
        # For large datasets, we'll generate data in batches to avoid overwhelming the AI
        batch_size = min(10, row_count)  # Generate up to 10 rows at a time
        num_batches = (row_count + batch_size - 1) // batch_size  # Ceiling division
        
        all_data = []
        
        for batch in range(num_batches):
            current_batch_size = min(batch_size, row_count - batch * batch_size)
            logger.info(f"Generating batch {batch+1}/{num_batches} ({current_batch_size} rows)")
            
            # Create a prompt for the AI to generate data
            system_prompt = """
    You are an expert data generator specializing in creating realistic synthetic datasets.
    Generate a batch of synthetic data rows based on the provided schema.
    
    Your task is to:
    1. Create realistic, diverse data that follows the schema specifications
    2. Ensure logical consistency between related columns
    3. Follow domain-specific rules
    4. Generate unique values for ID columns
    5. Return data in a valid JSON format that can be parsed
    
    Return a JSON array of objects, where each object represents one row of data.
    """
            
            # Prepare the user query with schema details
            user_query = f"""
    Schema Information:
    Dataset Name: {schema['dataset_name']}
    Description: {schema['description']}
    Columns: {', '.join(columns)}
    
    Column Details:
    {json.dumps(column_details, indent=2)}
    
    Domain-Specific Rules:
    {json.dumps(domain_rules, indent=2)}
    
    Generation Guidance:
    {schema.get('data_generation_guidance', '')}
    
    Batch Information:
    - Batch {batch+1} of {num_batches}
    - Generate {current_batch_size} rows of data
    - Start ID from {batch * batch_size + 1}
    
    Generate {current_batch_size} rows of realistic data following this schema and return as a JSON array.
    """
            
            messages = [
                SystemMessage(content=system_prompt),
                HumanMessage(content=user_query)
            ]
            
            response = await self.llm.ainvoke(messages)
            
            # Extract JSON array from response
            json_match = re.search(r'\[.*\]', response.content, re.DOTALL)
            if json_match:
                try:
                    batch_data = json.loads(json_match.group())
                    all_data.extend(batch_data)
                    logger.info(f"Successfully generated {len(batch_data)} rows")
                except json.JSONDecodeError as e:
                    logger.error(f"Failed to parse AI-generated data: {e}")
                    # Use fallback data generation for this batch
                    all_data.extend(self._generate_fallback_data(schema, current_batch_size, batch * batch_size))
            else:
                logger.warning("Failed to extract data from AI response, using fallback approach")
                # Use fallback data generation for this batch
                all_data.extend(self._generate_fallback_data(schema, current_batch_size, batch * batch_size))
        
        # Convert to DataFrame
        df = pd.DataFrame(all_data)
        
        # Ensure all expected columns are present
        for col in columns:
            if col not in df.columns:
                logger.warning(f"Column {col} missing in generated data, adding with placeholder values")
                df[col] = ["Unknown" for _ in range(len(df))]
        
        # Reorder columns to match schema
        df = df[columns]
        
        return df
    
    def _generate_fallback_data(self, schema: Dict[str, Any], batch_size: int, start_id: int) -> List[Dict[str, Any]]:
        """Generate fallback data if AI generation fails"""
        logger.info("Using fallback data generation")
        
        columns = schema['columns']
        column_details = schema.get('column_details', {})
        user_context = schema.get('user_context', {})
        domain = user_context.get('domain', 'general')
        
        data_rows = []
        
        for i in range(batch_size):
            row_id = start_id + i + 1
            row = {}
            
            for col in columns:
                col_detail = column_details.get(col, {})
                col_type = col_detail.get('type', 'string')
                
                # Generate value based on column type
                if 'id' in col.lower() or col_type == 'id':
                    # Generate unique IDs
                    if 'patient' in col.lower() or 'person' in col.lower():
                        row[col] = f"PAT_{row_id:06d}"
                    else:
                        row[col] = f"ID_{row_id:06d}"
                
                elif col_type == 'int' or 'age' in col.lower() or 'year' in col.lower():
                    # Generate integer values
                    if 'age' in col.lower():
                        row[col] = random.randint(18, 90)
                    elif 'year' in col.lower():
                        row[col] = random.randint(1950, 2023)
                    else:
                        row[col] = random.randint(0, 100)
                
                elif col_type == 'float' or any(metric in col.lower() for metric in ['score', 'index', 'ratio', 'rate']):
                    # Generate float values
                    row[col] = round(random.uniform(0, 100), 2)
                
                elif col_type == 'date':
                    # Generate dates
                    row[col] = fake.date_between(start_date='-5y', end_date='today').strftime('%Y-%m-%d')
                
                elif col_type == 'boolean':
                    # Generate boolean values
                    row[col] = random.choice([True, False])
                
                elif col_type == 'categorical':
                    # Generate categorical values based on column name
                    if 'sex' in col.lower() or 'gender' in col.lower():
                        row[col] = random.choice(['Male', 'Female'])
                    elif 'ethnicity' in col.lower() or 'race' in col.lower():
                        row[col] = random.choice(['White', 'Black', 'Asian', 'Hispanic', 'Other'])
                    elif 'frequency' in col.lower():
                        row[col] = random.choice(['Never', 'Rarely', 'Monthly', 'Weekly', 'Daily'])
                    else:
                        row[col] = f'Value_{random.randint(1, 5)}'
                
                else:  # Default to string/text
                    row[col] = fake.text(max_nb_chars=50)
            
            data_rows.append(row)
        
        return data_rows
    
    def _save_dataset_as_csv(self, dataset: GeneratedDataset):
        """Save the generated dataset as a CSV file"""
        df = self._dataframes.get(dataset.name)
        if df is not None:
            df.to_csv(dataset.file_path, index=False)
            logger.info(f"Saved dataset to {dataset.file_path}")

async def main():
    """Main execution function"""
    # Get Gemini API key from environment variable
    gemini_api_key = os.getenv('GEMINI_API_KEY')
    if not gemini_api_key:
        raise ValueError("GEMINI_API_KEY environment variable is required")
    
    # Initialize AI Dataset Generator
    generator = AIDatasetGenerator(gemini_api_key)
    
    # Process Agent 1's output
    agent1_output_path = "agent1_output.json"
    if not os.path.exists(agent1_output_path):
        raise FileNotFoundError(f"Agent 1 output file not found: {agent1_output_path}")
    
    print("\nAgent 2: AI-Driven Dataset Generation")
    print("=" * 50)
    print(f"Processing Agent 1 output: {agent1_output_path}")
    
    # Generate datasets
    result = await generator.process_agent1_output(agent1_output_path)
    
    # Print summary
    print("\nGeneration Complete!")
    print(f"Generated {len(result.generated_datasets)} datasets:")
    for dataset in result.generated_datasets:
        print(f"  - {dataset.name}: {dataset.row_count} rows, {len(dataset.columns)} columns")
        print(f"    Saved to: {dataset.file_path}")
    
    print(f"\nAgent 2 output saved to: {os.path.join(generator.output_dir, 'agent2_ai_output.json')}")
    print("Ready for Agent 3 processing")

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())